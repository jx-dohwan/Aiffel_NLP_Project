{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1O6TxYALkg2ebyz3qAqRcuXa-vlgmpmM6",
      "authorship_tag": "ABX9TyM57u76i/b3C3t0GASfrJ8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0ef9e40cb57457a84950bab4953169f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4de021d5ee094eecb0b91fa64a7ebf7d",
              "IPY_MODEL_954516d921f04056963500414f2dad8f",
              "IPY_MODEL_484c1ef0829f478da58561feac783e30"
            ],
            "layout": "IPY_MODEL_bb98f60f7ab848c2a8f896c72cc39051"
          }
        },
        "4de021d5ee094eecb0b91fa64a7ebf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919b1ae2572a40e7bdbf86bb2ec2561a",
            "placeholder": "​",
            "style": "IPY_MODEL_d4c1ce6b1405465b9074ff25b81a9d40",
            "value": "100%"
          }
        },
        "954516d921f04056963500414f2dad8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d53f317e7fa4c079be1f1a191698ebb",
            "max": 78968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18a3108fbf8f4075b9c7e18a9ca28aeb",
            "value": 78968
          }
        },
        "484c1ef0829f478da58561feac783e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c70effb461745169fa5b81aa74c8e65",
            "placeholder": "​",
            "style": "IPY_MODEL_f7b4e0bb62ef4533a637f30505b02817",
            "value": " 78968/78968 [00:03&lt;00:00, 24395.10it/s]"
          }
        },
        "bb98f60f7ab848c2a8f896c72cc39051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919b1ae2572a40e7bdbf86bb2ec2561a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c1ce6b1405465b9074ff25b81a9d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d53f317e7fa4c079be1f1a191698ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a3108fbf8f4075b9c7e18a9ca28aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c70effb461745169fa5b81aa74c8e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b4e0bb62ef4533a637f30505b02817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jx-dohwan/Aiffel_NLP_Project/blob/main/%5BNLP_10%5DCreate_a_translator_with_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [NLP_10]Create a translator with Transformer"
      ],
      "metadata": {
        "id": "OZN6XI2KaN8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import 및 라이브러리 다운로드"
      ],
      "metadata": {
        "id": "atEzWNwswXu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLAP75iryuwy",
        "outputId": "45084626-16e1-45b0-cfde-abab4d13f0d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 1s (9,693 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3AolEf_0eNr",
        "outputId": "415baa7d-77b9-4314-c032-4f7c441780e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7-21ud_ZaIWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c2aaa8-279b-40cb-dac1-90bd56785f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm  \n",
        "import seaborn # Attention 시각화를 위해 필요!\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터 다운로드"
      ],
      "metadata": {
        "id": "2a4vPdaAwbQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_train_ko = '/content/drive/MyDrive/인공지능/아이펠/GoingDeeper/제출용/data/transformer_translator/korean-english-park.train.ko'\n",
        "path_train_en = '/content/drive/MyDrive/인공지능/아이펠/GoingDeeper/제출용/data/transformer_translator/korean-english-park.train.en'"
      ],
      "metadata": {
        "id": "cwjKuxmfweMI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(path_train_ko, \"r\") as f:\n",
        "#     raw_ko = f.read().splitlines()\n",
        "\n",
        "# print(\"Data Size:\", len(raw_ko))\n",
        "# print(\"Example:\")\n",
        "\n",
        "# for sen in raw_ko[0:100][::20]: print(\">>\", sen)"
      ],
      "metadata": {
        "id": "9JJWJCLMzLLn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(path_train_en, \"r\") as f:\n",
        "#     raw_en = f.read().splitlines()\n",
        "\n",
        "# print(\"Data Size:\", len(raw_en))\n",
        "# print(\"Example:\")\n",
        "\n",
        "# for sen in raw_en[0:100][::20]: print(\">>\", sen)"
      ],
      "metadata": {
        "id": "5oZuS0B_zM7B"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 데이터 정제 및 토큰화\n",
        "\n",
        "1. set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
        "<br><br>\n",
        "\n",
        "2. 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
        "  - 모든 입력을 소문자로 변환합니다.\n",
        "  - 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
        "  - 문장부호 양옆에 공백을 추가합니다.\n",
        "  - 문장 앞뒤의 불필요한 공백을 제거합니다.\n",
        "<br><Br>\n",
        "\n",
        "3. 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행합니다! 토큰화에는 Sentencepiece를 활용하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 generate_tokenizer() 함수를 정의합니다. 최종적으로 ko_tokenizer 과 en_tokenizer 를 얻으세요. en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게 합니다.\n",
        "  - https://github.com/google/sentencepiece\n",
        "    - 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
        "    - 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.\n",
        "    - 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
        "PAD : 0 / BOS : 1 / EOS : 2 / UNK : 3\n",
        "<br><Br>\n",
        "\n",
        "4. 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환하세요! (❗모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다.)"
      ],
      "metadata": {
        "id": "filZHzQSwfMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) set 데이터형으로 변환"
      ],
      "metadata": {
        "id": "GYiDM6CU2OoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정제 및 토큰화\n",
        "def clean_corpus(kor_path, eng_path):\n",
        "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
        "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
        "    assert len(kor) == len(eng)\n",
        "\n",
        "    cleaned_corpus = list(set(zip(kor, eng)))\n",
        "\n",
        "    return cleaned_corpus\n",
        "\n",
        "cleaned_corpus = clean_corpus(path_train_ko, path_train_en)"
      ],
      "metadata": {
        "id": "BmlqerERcl6R"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned_corpus = list(set(zip(raw_ko, raw_en)))"
      ],
      "metadata": {
        "id": "o_v1KC8ZwgFA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2YX4yxszWbb",
        "outputId": "ac49ac6a-2daa-424d-b2e4-a1856d67e1e2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78968"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in cleaned_corpus[:10]:\n",
        "    print(pair)\n",
        "    print('pair 0 :',pair[0])\n",
        "    print('pair 1 :',pair[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx0ppTEP1mtR",
        "outputId": "4618fd4e-91e0-43b3-bf00-6e62088fdfc0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('20대때 영국에서 레이스를 만드는 직공으로 일했던 본저 라민에 대한 게시물은 90년 전 1차 세계대전 참호에서 보낸 내용이 담겨있다.', 'But the postings from the twentysomething English laceworker are from the trenches of World War I nine decades ago.')\n",
            "pair 0 : 20대때 영국에서 레이스를 만드는 직공으로 일했던 본저 라민에 대한 게시물은 90년 전 1차 세계대전 참호에서 보낸 내용이 담겨있다.\n",
            "pair 1 : But the postings from the twentysomething English laceworker are from the trenches of World War I nine decades ago.\n",
            "('그래도 GPS 기능에 방향 지시를 더한 것은 완전히 새로운 방향을 제시했다.', 'GPS plus pointing, though, suggests an entirely new direction.')\n",
            "pair 0 : 그래도 GPS 기능에 방향 지시를 더한 것은 완전히 새로운 방향을 제시했다.\n",
            "pair 1 : GPS plus pointing, though, suggests an entirely new direction.\n",
            "('한편 파업이 장기화 될 경우 20일 열리는 잉글랜드와 남아프리카공화국간의 럭비월드컵 결승전에 적지 않은 영향을 미칠 것으로 우려된다.', \"There are fears it could impact the thousands of rugby fans heading to Paris for Saturday's World Cup final between England and South Africa. Most international train services were still running, however.\")\n",
            "pair 0 : 한편 파업이 장기화 될 경우 20일 열리는 잉글랜드와 남아프리카공화국간의 럭비월드컵 결승전에 적지 않은 영향을 미칠 것으로 우려된다.\n",
            "pair 1 : There are fears it could impact the thousands of rugby fans heading to Paris for Saturday's World Cup final between England and South Africa. Most international train services were still running, however.\n",
            "('미국 관리들은 이 설명에 이의를 달지 않았다.', 'U.S. officials did not dispute the account.')\n",
            "pair 0 : 미국 관리들은 이 설명에 이의를 달지 않았다.\n",
            "pair 1 : U.S. officials did not dispute the account.\n",
            "('블랙은 지난 7월 우편 사기와 사법 방해등의 혐의로 유죄를 선고받았다.', 'Black, 63, was convicted in July of mail fraud and obstruction of justice.')\n",
            "pair 0 : 블랙은 지난 7월 우편 사기와 사법 방해등의 혐의로 유죄를 선고받았다.\n",
            "pair 1 : Black, 63, was convicted in July of mail fraud and obstruction of justice.\n",
            "('휴대폰 비교 웹사이트 오미오닷컴(Omio.com)의 어네스트 도쿠는 프리티시 일간 ‘프레스 어소시에이션(the British Press Association)’에서 “구글 브랜드가 유명하지만 G1의 디자인에는 애플이 거대시장에서 성공하게 만들어준 아이폰 3G 고유의 멋진 요소가 부족하다”고 밝혔다.', 'Ernest Doku, from mobile phone comparison Web site Omio.com, told the British Press Association: \"Despite the popularity of the Google brand, the G1\\'s design lacks the inherent \\'cool\\' factor that made the iPhone 3G such a mass market success for Apple.')\n",
            "pair 0 : 휴대폰 비교 웹사이트 오미오닷컴(Omio.com)의 어네스트 도쿠는 프리티시 일간 ‘프레스 어소시에이션(the British Press Association)’에서 “구글 브랜드가 유명하지만 G1의 디자인에는 애플이 거대시장에서 성공하게 만들어준 아이폰 3G 고유의 멋진 요소가 부족하다”고 밝혔다.\n",
            "pair 1 : Ernest Doku, from mobile phone comparison Web site Omio.com, told the British Press Association: \"Despite the popularity of the Google brand, the G1's design lacks the inherent 'cool' factor that made the iPhone 3G such a mass market success for Apple.\n",
            "('적자에 허덕이고 있는 거대 모기지기업인 프레디맥의 최고재무책임자 직무대행이 버지니아 북부의 자택에서 숨진 채 발견되었습니다.', 'The acting Chief Financial Officer of money-losing mortgage giant Freddie Mac has been found dead at his northern Virginia home.')\n",
            "pair 0 : 적자에 허덕이고 있는 거대 모기지기업인 프레디맥의 최고재무책임자 직무대행이 버지니아 북부의 자택에서 숨진 채 발견되었습니다.\n",
            "pair 1 : The acting Chief Financial Officer of money-losing mortgage giant Freddie Mac has been found dead at his northern Virginia home.\n",
            "('존 에밋 올림픽 조직위원장은 영국 의회 위원회에 출석, 런던 올림픽을 위해 책정된 27억 파운드의 추가 예산이 부족할 수도 있다고 주장했다.', 'John Armitt, the chairman of the Olympic Delivery Authority, told a British parliamentary committee that the 2.7-billion pound ($5.5B; euro3.7B) fund set aside for budget overspend may not be enough.')\n",
            "pair 0 : 존 에밋 올림픽 조직위원장은 영국 의회 위원회에 출석, 런던 올림픽을 위해 책정된 27억 파운드의 추가 예산이 부족할 수도 있다고 주장했다.\n",
            "pair 1 : John Armitt, the chairman of the Olympic Delivery Authority, told a British parliamentary committee that the 2.7-billion pound ($5.5B; euro3.7B) fund set aside for budget overspend may not be enough.\n",
            "('미국은 이라크는 모든 주권을 되찾게 되며 더 이상 미군의 점령 하에 있지 않을 것이지만 미군이 철수하는 데는 시간이 필요하다는 것을 이라크 국민에게 납득시킬 필요가 있다.', 'The United States needs to convince Iraqis that they will have full sovereignty and are no longer under occupation.')\n",
            "pair 0 : 미국은 이라크는 모든 주권을 되찾게 되며 더 이상 미군의 점령 하에 있지 않을 것이지만 미군이 철수하는 데는 시간이 필요하다는 것을 이라크 국민에게 납득시킬 필요가 있다.\n",
            "pair 1 : The United States needs to convince Iraqis that they will have full sovereignty and are no longer under occupation.\n",
            "('알카에다 소속의 한 단체가 13일(현지시간) 성명을 발표, 주말에 일어난 테러들이 자신들의 소행임을 밝히며, 자신들이 3명의 미군 포로를 잡고 있다고 주장했다.', 'BAGHDAD, Iraq (CNN) A terror group with links to al Qaeda in Iraq claimed responsibility Sunday for the weekend attack that sparked a manhunt for three missing U.S. soldiers, according to a statement on the Internet.')\n",
            "pair 0 : 알카에다 소속의 한 단체가 13일(현지시간) 성명을 발표, 주말에 일어난 테러들이 자신들의 소행임을 밝히며, 자신들이 3명의 미군 포로를 잡고 있다고 주장했다.\n",
            "pair 1 : BAGHDAD, Iraq (CNN) A terror group with links to al Qaeda in Iraq claimed responsibility Sunday for the weekend attack that sparked a manhunt for three missing U.S. soldiers, according to a statement on the Internet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 데이터 정제"
      ],
      "metadata": {
        "id": "xnxLErUCzbLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence, e_token=False):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "0pU-9je5zi_0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) SentencePiece"
      ],
      "metadata": {
        "id": "JklnNGeK2Fic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_corpus = []\n",
        "kor_corpus = []\n",
        "\n",
        "#num_examples = 20000\n",
        "\n",
        "for pair in cleaned_corpus:#[:num_examples]:\n",
        "    eng_corpus.append(preprocess_sentence(pair[0]))\n",
        "    kor_corpus.append(preprocess_sentence(pair[1]))\n",
        "\n",
        "print(\"한국어:\", eng_corpus[100])   # go away !\n",
        "print(\"영어:\", kor_corpus[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN3DXp040NwP",
        "outputId": "d51efc1c-467f-477e-acc3-a33fc6cb5a33"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어: 그러나 , 그들은 다수인 회교도와의 긴장관계에 대해 불평을 늘어놓고 , 특히 노동시장에서 차별받고 있다고 말하고 있다 .\n",
            "영어: but they complain of tensions with the muslim majority and say they face discrimination , particularly in the job market .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://paul-hyun.github.io/vocab-with-sentencepiece/\n",
        "- https://www.programcreek.com/python/example/117288/sentencepiece.SentencePieceProcessor"
      ],
      "metadata": {
        "id": "Y_P2UQxgFdoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokenizer(corpus, Mtype, vocab_size, ko_en):\n",
        "  temp_file = '/content/drive/MyDrive/인공지능/아이펠/GoingDeeper/제출용/data/transformer_translator/ratings_train_{}.txt.temp'.format(ko_en)\n",
        "  model_prefix = 'spm_{}'.format(ko_en)\n",
        "  with open(temp_file, 'w') as f:\n",
        "      for row in corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
        "          f.write(str(row) + '\\n')\n",
        "\n",
        "  if Mtype == 'bpe' or Mtype == 'unigram' or Mtype == 'char'or Mtype == 'word':\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        f\"--input={temp_file} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type={Mtype}\"+\n",
        "        \" --pad_id=0 --pad_piece=[PAD]\" + \n",
        "        \" --unk_id=1 --unk_piece=[UNK]\" + \n",
        "        \" --bos_id=2 --bos_piece=[BOS]\" + \n",
        "        \" --eos_id=3 --eos_piece=[EOS]\".format(temp_file, model_prefix, vocab_size, Mtype)    \n",
        "    )\n",
        "  else:\n",
        "    print(\"Mtype를 잘 못 입력 했습니다 'bpe', 'unigram', 'char', 'word' 중 하나를 입력해주세요\")\n",
        "    if __name__ == \"__main__\": # 참조 : https://www.delftstack.com/ko/howto/python/python-exit-if-statement/\n",
        "      load_data(0)  \n",
        "      print(\"Broken out\")\n",
        "  \n",
        "  #위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
        "\n",
        "  s = spm.SentencePieceProcessor()\n",
        "  s.Load(model_prefix+'.model')\n",
        "  if model_prefix == 'spm_en':\n",
        "    s.set_encode_extra_options(\"bos:eos\")\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "w0Mmn3cn2bCG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ko_tokenizer = generate_tokenizer(kor_corpus, 'unigram', 20000, 'ko')\n",
        "en_tokenizer = generate_tokenizer(eng_corpus, 'unigram', 20000, 'en')\n",
        "# SP_model_train('bpe', 8000)\n",
        "# SP_model_train('char', 8000)\n",
        "# SP_model_train('word', 8000)\n",
        "# SP_model_train('char', 16000)"
      ],
      "metadata": {
        "id": "q93kMyaN24qT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l spm_ko*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcxBXwJwAbwe",
        "outputId": "949851ba-fc6f-486a-daa4-39db2c428944"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 575645 Oct 13 09:01 spm_ko.model\n",
            "-rw-r--r-- 1 root root 355536 Oct 13 09:01 spm_ko.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l spm_en*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8-haG6Z3GvW",
        "outputId": "0b9d3d05-08b3-49f2-eb38-4cbec4d5645c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 623747 Oct 13 09:01 spm_en.model\n",
            "-rw-r--r-- 1 root root 403547 Oct 13 09:01 spm_en.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) 토큰화\n",
        "- https://lsjsj92.tistory.com/600\n",
        "- https://wikidocs.net/86657"
      ],
      "metadata": {
        "id": "GT3rR4XlL-Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_corpus = []\n",
        "tgt_corpus = []\n",
        "\n",
        "assert len(kor_corpus) == len(eng_corpus)\n",
        "\n",
        "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
        "for idx in tqdm(range(len(kor_corpus))):\n",
        "        ko_pair = ko_tokenizer.encode_as_ids(kor_corpus[idx])\n",
        "        en_pair = en_tokenizer.encode_as_ids(eng_corpus[idx])\n",
        "        if len(ko_pair) >= 50 or len(en_pair) >= 52:\n",
        "            src_corpus.append(ko_pair)\n",
        "            tgt_corpus.append(en_pair)\n",
        "\n",
        "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
        "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
        "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a0ef9e40cb57457a84950bab4953169f",
            "4de021d5ee094eecb0b91fa64a7ebf7d",
            "954516d921f04056963500414f2dad8f",
            "484c1ef0829f478da58561feac783e30",
            "bb98f60f7ab848c2a8f896c72cc39051",
            "919b1ae2572a40e7bdbf86bb2ec2561a",
            "d4c1ce6b1405465b9074ff25b81a9d40",
            "8d53f317e7fa4c079be1f1a191698ebb",
            "18a3108fbf8f4075b9c7e18a9ca28aeb",
            "4c70effb461745169fa5b81aa74c8e65",
            "f7b4e0bb62ef4533a637f30505b02817"
          ]
        },
        "id": "2zaWohVtfFHA",
        "outputId": "f63b126b-0173-400a-9bf5-5675ce49e0e0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/78968 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0ef9e40cb57457a84950bab4953169f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터와 검증 데이터로 분리하기\n",
        "enc_train, enc_test, dec_train, dec_test = \\\n",
        "train_test_split(enc_train, dec_train, test_size=0.2)\n",
        "\n",
        "print(\"한국어 train Size:\", len(enc_train))\n",
        "print(\"영어 train Size:\", len(dec_train))\n",
        "print(\"한국어 test Size:\", len(enc_test))\n",
        "print(\"영어 test Size:\", len(dec_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vsbNTj6kIjh",
        "outputId": "15a2edb4-6ede-4362-cb73-6f3d146d609d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 train Size: 5086\n",
            "영어 train Size: 5086\n",
            "한국어 test Size: 1272\n",
            "영어 test Size: 1272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src_corpus = []\n",
        "# tgt_corpus = []\n",
        "\n",
        "# num_examples = 20000\n",
        "\n",
        "# for pair in cleaned_corpus[:num_examples]:\n",
        "#     ko_pair = ko_tokenizer.encode_as_pieces(pair[0])\n",
        "#     en_pair = en_tokenizer.encode_as_pieces(pair[1])\n",
        "#     if len(ko_pair) >= 50 or len(en_pair) >= 52:\n",
        "#         src_corpus.append(ko_pair)\n",
        "#         tgt_corpus.append(en_pair)\n",
        "\n",
        "# print(\"한국어:\", src_corpus[100])   # go away !\n",
        "# print(\"영어:\", tgt_corpus[100])"
      ],
      "metadata": {
        "id": "Xg8LNlBnZQ9a"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def tokenize(corpus):\n",
        "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "#     tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "#     tensor = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "#     return tensor, tokenizer"
      ],
      "metadata": {
        "id": "LVjipDUDaZ4o"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 토큰화하기\n",
        "# enc_tensor, enc_tokenizer = tokenize(src_corpus)\n",
        "# dec_tensor, dec_tokenizer = tokenize(tgt_corpus)\n",
        "\n",
        "# # 훈련 데이터와 검증 데이터로 분리하기\n",
        "# enc_train, enc_val, dec_train, dec_val = \\\n",
        "# train_test_split(enc_train, dec_train, test_size=0.2)\n",
        "\n",
        "# print(\"한국어 Vocab Size:\", len(enc_tokenizer.index_word))\n",
        "# print(\"영어 Vocab Size:\", len(dec_tokenizer.index_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJat6LxNaazg",
        "outputId": "27c7b643-33fb-422d-940d-f956faca73ad"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 Vocab Size: 18165\n",
            "영어 Vocab Size: 12724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 모델 설계"
      ],
      "metadata": {
        "id": "F2xkL8hAwhzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "    return sinusoid_table"
      ],
      "metadata": {
        "id": "PclniQ0lwinw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "            \n",
        "        self.depth = d_model // self.num_heads\n",
        "            \n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
        "\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "            \n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "        \n",
        "    def call(self, Q, K, V, mask):\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "        \n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "            \n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask)\n",
        "    \t\t\t\t        \n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "                \n",
        "        return out, attention_weights"
      ],
      "metadata": {
        "id": "6EDEJBlWb-r4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.w_1(x)\n",
        "        out = self.w_2(out)\n",
        "            \n",
        "        return out"
      ],
      "metadata": {
        "id": "LZL55rB5cBMx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        return out, enc_attn"
      ],
      "metadata": {
        "id": "iyy8GZ4RcC9W"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "    \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Masked Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn\n",
        " "
      ],
      "metadata": {
        "id": "rZqwP2v3cEn8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                        for _ in range(n_layers)]\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "    \n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "        \n",
        "        return out, enc_attns"
      ],
      "metadata": {
        "id": "VlVGdecGcGKY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                            for _ in range(n_layers)]\n",
        "                            \n",
        "                            \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "    \n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "jM_9BCGOcHN4"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    src_vocab_size,\n",
        "                    tgt_vocab_size,\n",
        "                    pos_len,\n",
        "                    dropout=0.2,\n",
        "                    shared=True):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared = shared\n",
        "\n",
        "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "        \n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "        \n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "        \n",
        "        logits = self.fc(dec_out)\n",
        "        \n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "eU0mJOHycIkZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask"
      ],
      "metadata": {
        "id": "7UfzccdycKri"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch, length = 16, 20\n",
        "src_padding = 5\n",
        "tgt_padding = 15\n",
        "\n",
        "src_pad = tf.zeros(shape=(batch, src_padding))\n",
        "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
        "\n",
        "sample_data = tf.ones(shape=(batch, length))\n",
        "\n",
        "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
        "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
        "\n",
        "enc_mask, dec_enc_mask, dec_mask = \\\n",
        "generate_masks(sample_src, sample_tgt)\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "\n",
        "ax1 = fig.add_subplot(131)\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax3 = fig.add_subplot(133)\n",
        "\n",
        "ax1.set_title('1) Encoder Mask')\n",
        "ax2.set_title('2) Encoder-Decoder Mask')\n",
        "ax3.set_title('3) Decoder Mask')\n",
        "\n",
        "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
        "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
        "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "c727t9chcNJ9",
        "outputId": "341d0de2-2428-451e-bea6-4edfba9b5bd0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = LearningRateScheduler(512)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "P7bjhGDHcPfs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 훈련하기\n",
        "\n",
        "1. 2 Layer를 가지는 Transformer를 선언하세요.\n",
        "(하이퍼파라미터는 자유롭게 조절합니다.)\n",
        "<br><br>\n",
        "2. 논문에서 사용한 것과 동일한 Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언하세요. (Optimizer의 파라미터 역시 논문과 동일하게 설정합니다.)\n",
        "<br><br>\n",
        "3. Loss 함수를 정의하세요.\n",
        "Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, Masking 되지 않은 입력의 개수로 Scaling하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)\n",
        "<br><br>\n",
        "4. train_step 함수를 정의하세요.\n",
        "입력 데이터에 알맞은 Mask를 생성하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다.\n",
        "<br><br>\n",
        "5. 학습을 진행합니다.\n",
        "매 Epoch 마다 제시된 예문에 대한 번역을 생성하고, 멋진 번역이 생성되면 그때의 하이퍼파라미터와 생성된 번역을 제출하세요!\n"
      ],
      "metadata": {
        "id": "xp2_SYBDwji9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 회고"
      ],
      "metadata": {
        "id": "DPFeI0yKyG2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.루브릭 기준"
      ],
      "metadata": {
        "id": "7H63NMbbyJUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.\n",
        "    - 데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.\n",
        "\n",
        "2. Transformer 번역기 모델이 정상적으로 구동된다.\n",
        "    - Transformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.\n",
        "    \n",
        "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\n",
        "    - 제시된 문장에 대한 그럴듯한 영어 번역문이 생성되며, 시각화된 Attention Map으로 결과를 뒷받침한다."
      ],
      "metadata": {
        "id": "eoGSNBUFyMa0"
      }
    }
  ]
}